{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pulp\n",
    "import sys\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '../data/episodic/MDP10.txt'\n",
    "fl = open(fname , 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = 'lp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5\n"
     ]
    }
   ],
   "source": [
    "S = int(fl.readline())\n",
    "A = int(fl.readline())\n",
    "print(S , A)\n",
    "T = np.zeros([S,A,S])\n",
    "R = np.zeros([S,A,S])\n",
    "\n",
    "for i in range(S):\n",
    "    for j in range(A):\n",
    "        ln = fl.readline()\n",
    "        R[i,j,:] = np.array([float(k) for k in ln.strip().split('\\t')])\n",
    "\n",
    "for i in range(S):\n",
    "    for j in range(A):\n",
    "        ln = fl.readline()\n",
    "        T[i,j,:] = np.array([float(k) for k in ln.strip().split('\\t')])\n",
    "\n",
    "gamma = float(fl.readline())\n",
    "gamma\n",
    "\n",
    "typ = fl.readline()\n",
    "typ = str(typ.strip())\n",
    "typ\n",
    "\n",
    "fl.close()\n",
    "\n",
    "RT = R*T\n",
    "\n",
    "# --- CHECK if the type is continuing\n",
    "\n",
    "# --- Howard policy itereation method for now\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def V_calc(T,RT,S,policy,gamma,typ = 'continuing'):\n",
    "    a_ = T[range(S),policy,:]\n",
    "    b_ = RT[range(S),policy,:]\n",
    "    b_ = np.sum(b_ , axis = 1)\n",
    "    a_ = np.eye(S) - (gamma * a_)\n",
    "    if typ == 'episodic':\n",
    "        a_ = a_[:-1,:-1]\n",
    "        b_ = b_[:-1]\n",
    "    V = np.linalg.solve(a_,b_)\n",
    "    if typ == 'episodic':\n",
    "        V = np.pad(V, (0, 1), 'constant')\n",
    "    return V\n",
    "\n",
    "def Policy_calc(T,RT,S,V,gamma,typ = 'continuing'):\n",
    "    Q_imd = RT + gamma * T * V.reshape([1,1,S]) \n",
    "    Q = np.sum(Q_imd,axis=2)\n",
    "    new_policy = np.argmax(Q,axis = 1)\n",
    "    return new_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry\n"
     ]
    }
   ],
   "source": [
    "best_policy = np.random.randint(0,A,S)\n",
    "best_value = np.random.rand(S)\n",
    "\n",
    "if algo == 'hpi':\n",
    "    policy = np.random.randint(0,A,S)\n",
    "\n",
    "    max_iter = 10\n",
    "    # while True:\n",
    "    for iterr in range(max_iter): \n",
    "        V = V_calc(T,RT,S,policy,gamma)\n",
    "        new_policy = Policy_calc(T,RT,S,V,gamma)\n",
    "    #     print(list(zip(V,new_policy)))\n",
    "        if np.sum(policy == new_policy) == S:\n",
    "            best_policy = new_policy\n",
    "            best_value = V_calc(T,RT,S,new_policy,gamma)\n",
    "            best_policy = best_policy.flatten()\n",
    "            best_value = best_value.flatten()\n",
    "            break\n",
    "        policy = new_policy\n",
    "else:\n",
    "    prob = pulp.LpProblem('MDP' , pulp.LpMinimize)\n",
    "    val_list = []\n",
    "    S_temp = S\n",
    "    if typ == 'episodic':\n",
    "        S_temp = S - 1\n",
    "    for i in range(S_temp):\n",
    "        variabl = 'V' + str(i)\n",
    "        variabl = pulp.LpVariable(variabl)\n",
    "        val_list.append(variabl)\n",
    "    prob += pulp.lpSum(val_list), \"Main Objective\"\n",
    "    ## Constraint\n",
    "    RT_summed = np.sum(RT,axis = 2)\n",
    "    for s in range(S_temp):\n",
    "        for a in range(A):\n",
    "            prob += pulp.lpSum([ T[s,a,s_p] * val_list[s_p] for s_p in range(S_temp)]) * gamma + RT_summed[s,a] <=val_list[s] , \"for each s = \"+ str(s)+\",a = \"+ str(a)+\" constraint\" \n",
    "    prob.solve()\n",
    "    best_value = np.zeros([S,]).flatten()\n",
    "    for v in prob.variables():\n",
    "        best_value[int(str(v.name)[1:])] = v.varValue\n",
    "    best_policy = Policy_calc(T,RT,S,best_value,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.229758700000000\t2\n",
      "0.968345610000000\t0\n",
      "0.824782670000000\t2\n",
      "1.390925000000000\t0\n",
      "1.162834000000000\t4\n",
      "1.053308000000000\t3\n",
      "0.949532020000000\t1\n",
      "1.013942300000000\t2\n",
      "1.074288400000000\t3\n",
      "0.000000000000000\t0\n"
     ]
    }
   ],
   "source": [
    "for iterr in range(S):\n",
    "    print(\"{:.15f}\\t{}\".format(best_value[iterr] , best_policy[iterr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = pulp.LpProblem('MDP' , pulp.LpMinimize)\n",
    "\n",
    "val_list = []\n",
    "S_temp = S\n",
    "if typ == 'episodic':\n",
    "    S_temp = S - 1\n",
    "for i in range(S_temp):\n",
    "    variabl = 'V' + str(i)\n",
    "    variabl = pulp.LpVariable(variabl)\n",
    "    val_list.append(variabl)\n",
    "\n",
    "\n",
    "prob += pulp.lpSum(val_list), \"Main Objective\"\n",
    "\n",
    "RT_summed = np.sum(RT,axis = 2)\n",
    "## Constraint\n",
    "\n",
    "for s in range(S_temp):\n",
    "    for a in range(A):\n",
    "        prob += pulp.lpSum([ T[s,a,s_p] * val_list[s_p] for s_p in range(S_temp)]) * gamma + RT_summed[s,a] <=val_list[s] , \"for each s = \"+ str(s)+\",a = \"+ str(a)+\" constraint\" \n",
    "\n",
    "prob.solve()\n",
    "\n",
    "best_value = np.zeros([S,]).flatten()\n",
    "for v in prob.variables():\n",
    "    best_value[int(str(v.name)[1:])] = v.varValue\n",
    "best_policy = Policy_calc(T,RT,S,best_value,gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu] *",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
